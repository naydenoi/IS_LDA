{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task: building a books recommendation engine\n",
    "* could to be written by Andrew (a motivation on doing a recommendation engine)*\n",
    "\n",
    "A books recommendation system aims to help users findng books which might be interesting for them based on book titles .....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is LDA? \n",
    "\n",
    "In order to understand the kind of books theat a certain user likes to read, we used a natural language processing technique called Latent Dirichlet Allocation (LDA )used foridentifing  hidden topics of documents based on the co-occurrence of words collected from those documents. \n",
    "\n",
    "The general idea of LDA is that \n",
    ">each document is generated from a mixture of topics and each of those topics is a mixture of words.\n",
    "\n",
    "Having this in mind, one could create a mechanism for generating new documents, i.e. we know the topics a priori, or for inferring topics present in a set of documents whicih is already known for us. This baysian topic modelling technique can be used to find out how high is the share of a certain  document devoted to a particular topic, which allows the recommendation system to categorize a book topic, for instance, as 30% thriller and 20% politics.\n",
    "\n",
    "Concerning the model name, one can think of it as follows (...):\n",
    "\n",
    "`Latent`: Topic structures in a document are latent meaning they are hidden structures in the text.\n",
    "\n",
    "`Dirichlet`: The Dirichlet distribution determines the mixture proportions of the topics in the documents and the words in each topic.\n",
    "\n",
    "`Allocation` : Allocation of words to a given topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Parameter esmimation \n",
    "\n",
    "LDA is a generative probabilistic model, so to understand exactly how this works we need to understand the underlying probability distributions. \n",
    "\n",
    "The idea behind probabilistic modeling is (Blei, Ng, and Jordan 2003): \n",
    "  - to treat  data as observations that arise from some kind of  generative probabilistic  process (the hidden variables reflect the thematic structure of the documents (books) collection), \n",
    "  - to infer the hidden structure using posterior inference (What are the topics that describe this collection?) and \n",
    "  - to situate new data into the estimated model (How does a new document fit into the estimated topic structure?)\n",
    "  \n",
    "In the next *XXX* sectios we will focus on the multinomial and Dirichlet distributions utilized by LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: The Building Blocks\n",
    "\n",
    ".....\n",
    "\n",
    "### Maximul likelihood  \n",
    "\n",
    "One of the simplest method of parameter estimation is the Maximum likelihood (ML) method. Effectively one can calculate the parameter $\\theta$ that maximizes the likelihood: \n",
    "\n",
    "*to be described further by Quang*\n",
    "\n",
    "### Bayesian Inference (building blocks)\n",
    "\n",
    "Further method for estimating parameters is to estimate the posterior of the distribution via Bayesian inference.\n",
    "\n",
    "*to be described further by Quang*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Multinomial Distribution\n",
    "\n",
    "Instead of maximum-likelihood, Bayesian inference encourages the use of predictive densities and evidence scores. This is illustrated in the context of the multinomial distribution, where predictive estimates are often used but rarely described as Bayesian (Minka, 2003).\n",
    "\n",
    "Now we will dicribe the multinomial distribution which is used to model the probability of words in a document. To this reasen we will also discuss the conjugate prior for the multinomial distributoin, the `Dirichlet distribution`.\n",
    "\n",
    "......\n",
    "\n",
    ".. to do : decribe the intuition behind the MD ...\n",
    "\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dirichlet distribution\n",
    "\n",
    "The conjugate prior for the multinomial distribution is the Dirichlet distribution. Similar to the beta distribution, Dirichlet can be thought of as a distribution of distributions. Also note that the beta distribution is the special case of a Dirichlet distribution where the number of possible outcome is 2. This is similar to the relationship between the binomial and multinomial distributions.\n",
    "\n",
    "The probability distribution function for the Dirichlet distribution is shown in Equation (3.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By using an entropy approximation to the evidence, many Bayesian quantities can be ex- pressed in information-theoretic terms. For example, testing whether two samples come from the same distribution or testing whether two variables are independent boils down to a mutual information score (with appropriate smoothing). The same analysis can be applied to discrete Markov chains to get a test for Markovianity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generative process\n",
    "LDA is being often described as the simplest topic model (...): The intuition behind this baysian model is that documents exhibit multiple topics. Furthermore, one could most easily describe the  model  by its generative process, by which the model assumes the documents in the collection arose (...). \n",
    "\n",
    "Let's assume that a topic is a distribution over a fixed vocabulary and these topics are specified before any data has been generated. First of all, we generate the words in a two-stage process for each book in the the whole data collection:\n",
    "\n",
    "1. Randomly choose a distribution over topics.\n",
    "2. For each word in the document\n",
    "   -  A topic is being randomly choosen from the distribution over topics in the first step.\n",
    "   -  A word is being randomly choosen from the corresponding distribution over the vocabulary.\n",
    "\n",
    "This statistical model reflects the intuition that documents exhibit multiple\n",
    "topics. Each document exhibits the topics in different proportion (step #1); each word in each document is drawn from one of the topics (step #2b), where the selected topic is\n",
    "chosen from the per-document distribution over topics (step #2a).b\n",
    "\n",
    "\n",
    "The  distingnive characteristic of LDA is that all the documents in the collection share the same set of topics, but each document exhibits those topics in different proportion. As previuosly mentioned, topic modeling aims to automatically discover the topics from a collection of documents, which are observed, while the topic structure—the topics, per-document topic distributions, and the per-document per-word topic assignments has hidden structure.  The central computational problem for topic modeling is to use the observed documents to infer the hidden topic structure. This can be thought of as\n",
    "“reversing” the generative process— what is the hidden structure that likely generated the observed collection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA formally described \n",
    "LDA can be decsribed more formally with the following notation:\n",
    "\n",
    "<img align=\"middle\" src=\"imagesLDA/pic7.png\"> \n",
    "\n",
    "- The `topics` are $b_{1}:K$, where each $b_{k}$ is a distribution over the vocabulary. \n",
    "\n",
    "- The `topic proportion` for the $d^{th}$ document are anotated by $\\theta_{d}$ where $\\theta_{d,k}$ is the `topic proportion` for `topic` $k$ in `document` $d$.\n",
    "\n",
    "- The `topic assignment` for the $d^{th}$ document are $z_{d}$, where $z_{d,n}$ is the topic assignment for the $n^{th}$  word in document *d*.\n",
    "\n",
    "- The observed `words` for document *d* are $w_{d}$, where $w_{d,n}$ is the $n^{th}$ word in document *d*, which is an element from the fixed vocabulary.\n",
    "\n",
    "The notation on the figure is called plate notation which is used to create visual representation of the Bayesian Network: ..... continue \n",
    "\n",
    "\n",
    "Using this notation, the generative process for LDA  is equivalent  to the following joint distribution of the hidden and observed variables (...):\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\beta_{1:K} , \\theta_{1:D} , z_{1:D} , w_{1:D}) =\\displaystyle\\prod_{i=1}^{K}p(\\beta_{i})\\displaystyle\\prod_{d=1}^{D}p(\\theta_{d})\\Bigg( \\displaystyle\\prod_{n=1}^{N}p(z_{d,n}\\mid\\theta_{d}) p(w_{d,n}\\mid\\beta_{1:k}, z_{d,n} )\\Bigg)\n",
    "\\end{equation*}\n",
    "\n",
    "As you can see from this distribution the `topic assignmen` $z_{d,n}$\n",
    "depends on the `per-document topic distributio` $\\theta_{d}$, and the word $w_{d,n}$ depends on all of the `topics` $\\beta_{1:K}$ and on the `topic assignmen` $z_{d,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ###  A central research goal of modern probabilistic modeling is to develop efficient methods for approximating the posterior inference (...). \n",
    "\n",
    "Before we turn to the the computational problem of computing the conditional distribution of the topic structure given the observed documents (called the posterior)\n",
    "\n",
    "we Using our notation, the posterior is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " \n",
    "###  Posterior computation for LDA.\n",
    "\n",
    "\n",
    "\n",
    "Posterior computation for LDA (OR INFERENCE nennen) \n",
    "We now turn to the computational problem, computing the conditional distribution of the topic structure given the observed documents. (As we mentioned, this is called the posterior.) Using our notation, the posterior is \n",
    "\n",
    "\n",
    "The numerator is the joint distribution of all the random variables, which can be easily computed for any setting of the hidden variables. The denominator is the marginal probability of the observations, which is the probability of seeing the observed corpus under any topic model. In theory, it can be computed by summing the joint distribution over every possible instantiation of the hidden topic structure.\n",
    "\n",
    "That number of possible topic structures, however, is exponentially large; this sum is intractable to compute.\n",
    "\n",
    "A central research goal of modern probabilistic modeling is to develop efficient methods for approximating it. All is not lost, however, as there are a number of approximate inference techniques available that we can apply to the problem including variational inference (as used in the original LDA paper) and Gibbs Sampling (as we will use here).\n",
    "\n",
    "3.1 Gibbs Sampling\n",
    "3.1.1 Theory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
