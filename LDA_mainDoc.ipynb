{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a LDA-based Book Recommender System\n",
    "#### Autors: Andrew McAllister, Ivana Naydenova, Quang "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" src=\"introPic.png\"> \n",
    "[Source: http://people.ischool.berkeley.edu/~vivienp/presentations/is296/ass1nonfiction.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task: Building a books recommendation engine\n",
    "\n",
    "Latent Dirichlet Allocation is a type of unobserved learning algorithm in which topics are inferred from a dictionary of text corpora whose structures are not known (are latent). The practical use of such an algorithm is to solve the cold-start problem, whereby analytics can be done on texts to derive similarities in the dictionary's corpses, and further be expanded to others outside the original scope - all without user input, which is often an early stumbling block for businesses that do not meet early usage quotas to derive needed analysis. \n",
    "\n",
    "What we will be applying this to towards the end of this article is a recommendation engine for books based on their Wikipedia articles. Our recommendation engine aims to help users find books which might be interesting for them based on their summaries. At this point the algorithm is fully content based, lacking user input for collaborative filtering completely, but serves to illustrate the potential of such algorithms in forming the basis of high-level business implementable hybrid filters.\n",
    "\n",
    "The data for the project - all books on Wikipedia - is collected from Wikipedia Dumps from the 1st of January, 2019, in their compressed forms. Using techniques outlined by Will Koehrsen of Medium's Towards Data Science  we use a process of XML handlers to separate out individual pages, myparserfromhell's Wikipedia parser to derive the article titles, article contents, and length of each article, and further paralyze the process to assure optimal speed when going through all 15.8 GBs of compressed data ([Link to the original article]( https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c)). Our thanks to Mr. Koehrsen for his techniques on how to access and create such an interesting data set for our analysis.\n",
    "\n",
    "What follows is an overview of the related work on this topic followed by an explanation of the theoretics behind LDA to set the groundwork of our implementation. At the end, we will work through the full codes and present specific results of the algorithm, as well as discuss some of its shortcomings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work \n",
    "\n",
    "The Latent Dirichlet Allocation (LDA) model and the Variational EM algorithm used for training the model were proposed by Blei, Ng and Jordan in 2003 (Blei et al., 2003a). Blei et al. (2003b) described LDA as a “generative probabilistic model of a corpus which idea is that the documents are represented as weighted relevancy vectors over latent topics, where a distribution over words characterizes a topic”. This topic model belongs to the family of hierarchical Bayesian models of a corpus, which has the purpose to expose the main themes of a corpus that can be used to classify, search, and investigate the documents of the corpus. \n",
    "In LDA models, a topic is a distribution over the feature space of the corpus, and several topics with different weights can represent each document. According to Blei et al. (2003a), the number of topics (clusters) and the proportion of vocabulary that create each topic (the number of words in a cluster) are considered to be tbe two hidden variables of the model. The conditional distribution of words in topics, given these variables for an observed set of documents, is regarded as the primary challenge of the model.\n",
    "\n",
    "Griffiths and Steyvers (2004), used a derivation of the Gibbs sampling algorithm for learning LDA models to analyze abstracts from  [PNAS](https://www.pnas.org) by using Bayesian model selection to set the number of topics. They proved that the extracted topics capture essential structure in the data, and are further compatible with the class designations provided by the authors of the articles. They drew further applications of this analysis, including identifying ‘‘hot topics’’ by examining temporal dynamics and tagging abstracts to illustrate semantic content. The work of Griffiths and Steyvers (2004) proved the Gibbs sampling algorithm is more efficient than other LDA training methods (e.g., Variational EM). The efficiency of the Gibbs sampling algorithm for inference in a variety of models that extend LDA is associated with the \"the conjugacy between the Dirichlet distribution and the multinomial likelihood\". Thus, when one does sampling, the posterior sampling becomes easier, because the posterior distribution is the same as the prior, and it makes inference feasible. Therefore, when we are doing sampling, the posterior sampling becomes easier. (Blei et al., 2009; McCallum et al., 2005).\n",
    "\n",
    "Mimno et al. (2012) introduced a hybrid algorithm for Bayesian topic modeling, in which the main effort is to merge the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. Their approach decreases the bias of variational inference and can be generalized by many Bayesian hidden-variable models.\n",
    "\n",
    "LDA is being applied in various Natural Language Processing tasks such as for opinion analysis (Zhao et al., 2010), for native language identification (Jojo et al., 2011) and for learning word classes (Chrupala, 2011).\n",
    "\n",
    "In this blog post, we focus on the task of automatic text classification into a set of  topics using LDA, in order to help users find books which might be interesting for them based on their summaries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is LDA? \n",
    "\n",
    "In order to understand the kind of books that a certain user likes to read, we used a natural language processing technique called Latent Dirichlet Allocation (LDA )used for identifying hidden topics of documents based on the co-occurrence of words collected from those documents.\n",
    "\n",
    "The general idea of LDA is that \n",
    ">each document is generated from a mixture of topics and each of those topics is a mixture of words.\n",
    "\n",
    "Having this in mind, one could create a mechanism for generating new documents, i.e. we know the topics a priori, or for inferring topics present in a set of documents which is already known for us. This bayesian topic modelling technique can be used to find out how high the share of a certain document devoted to a particular topic is, which allows the recommendation system to categorize a book topic, for instance, as 30% thriller and 20% politics. These topics will not and do not have to be explicitly defined. Managers looking to apply LDA will often expect that outputs of specific topic classes will be provided by the analysis, but this is often extraneous - or potentially harmful to the robust potential of the analysis when a topic set is chosen such that they can be easily defined by human intuition. In our analysis, we will directly name any of our found topics, but will draw the reader's attention to when the words do seem to fit in a potential topic.\n",
    "\n",
    "Concerning the model name, one can think of it as follows (...):\n",
    "\n",
    "`Latent`: Topic structures in a document are latent meaning they are hidden structures in the text.\n",
    "\n",
    "`Dirichlet`: The Dirichlet distribution determines the mixture proportions of the topics in the documents and the words in each topic.\n",
    "\n",
    "`Allocation`: Allocation of words to a given topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Parameter estimation \n",
    "\n",
    "LDA is a generative probabilistic model, so to understand exactly how it works one needs first to understand the underlying probability distributions.\n",
    "The idea behind probabilistic modeling is (Blei, Ng, and Jordan 2003):\n",
    "- to treat data as observations that arise from some kind of generative probabilistic process (the hidden variables reflect the thematic structure of the documents (books) collection),\n",
    "- to infer the hidden structure using posterior inference (What are the topics that describe this collection?) and\n",
    "- to situate new data into the estimated model (How does a new document fit into the estimated topic structure?)\n",
    "\n",
    "In the next sectios we will focus on the multinomial and Dirichlet distributions utilized by LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference: The Building Blocks\n",
    "\n",
    ".....\n",
    "\n",
    "### Maximum likelihood  \n",
    "\n",
    "One of the simplest methods of parameter estimation is the Maximum likelihood (ML) method. Effectively one can calculate the parameter $\\theta$ that maximizes the likelihood: \n",
    "\n",
    "*to be described further by Quang*\n",
    "\n",
    "### Bayesian Inference (building blocks)\n",
    "\n",
    "A further method for estimating parameters is to estimate the posterior of the distribution via Bayesian inference.\n",
    "\n",
    "*to be described further by Quang*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multinomial Distribution\n",
    "\n",
    "Instead of maximum-likelihood, Bayesian inference encourages the use of predictive densities and evidence scores. This is illustrated in the context of the multinomial distribution, where predictive estimates are often used but rarely described as Bayesian (Minka, 2003).\n",
    "\n",
    "Now we will describe the multinomial distribution which is used to model the probability of words in a document. For this reason, we will also discuss the conjugate prior for the multinomial distribution, the `Dirichlet distribution`.\n",
    "\n",
    "......\n",
    "\n",
    ".. to do : describe the intuition behind the MD ...\n",
    "\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet distribution — what is it, and why is it useful?\n",
    "\n",
    "The Dirichlet distribution is used for representation of the distributions over the simplex (the set of N-vectors whose components sum to 1) and  is a useful prior distribution on discrete probability distributions over categorical variables.\n",
    "\n",
    "It is a conjugate prior to the categorical and multinomial distributions (meaning that multiplying a Dirichlet prior by a multinomial or categorical likelihood will yield another Dirichlet distribution of a certain form). However, the primary difference between Dirichlet and multinomial distributions is that Dirichlet random variables are real-valued, where each element of the vector is in the interval $[0, 1]$, and multinomial random variables are integer-valued.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution defines a probability density for a vector valued input having the same characteristics as our multinomial parameter $\\theta$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Dir (\\vec\\theta\\mid \\vec\\alpha)={\\frac {\\Gamma (\\Sigma_{i=1}^{K}\\alpha_{i})}{\\prod _{i=1}^{K}\\Gamma (\\alpha_{i})}}\\prod _{i=1}^{K}\\theta_{i}^{\\alpha_{i}-1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "This equation  is often represented using the Beta function in place of the first term as seen below:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Dir (\\vec\\theta\\mid \\vec\\alpha)={\\frac {1}{B(\\alpha)}}\\prod _{i=1}^{K}\\theta_{i}^{\\alpha_{i}-1}\n",
    "\\end{equation}\n",
    "$$\n",
    "Where:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac {1}{B(\\alpha)} = {\\frac {\\Gamma (\\Sigma_{i=1}^{K}\\alpha_{i})}{\\prod _{i=1}^{K}\\Gamma (\\alpha_{i})}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "The Dirichlet distribution is parameterized by the vector $\\alpha$, which has the same number of elements *K* as our multinomial parameter $\\theta$ (Thomas Boggs, 2014). Thus, one could interpret $p(\\theta \\mid\\alpha)$ as responding to the question “what is the probability density associated with multinomial distribution $\\theta$, given that our Dirichlet distribution has parameter $\\alpha$.\n",
    "\n",
    "To get a better sense of what the distributions look like, let us visualize a few examples in the context of topic modeling. \n",
    "\n",
    "The Dirichlet distribution takes a number ($\\alpha$) for each topic. The GIF below shows the iterations of taking 1000 samples from a Dirichlet distribution using an increasing alpha value. Every topic gets the same alpha value one could see displayed, and the dots represent  a  distribution or mixture of the three topics having certain coordinates such as (0.6, 0.0, 0.4) or (0.4, 0.3, 0.3). Low $\\alpha$-values account for the case where most of the topic distribution samples are in the corners (near the topics). Having values (1.0, 0.0, 0.0), (0.0, 1.0, 0.0), or (0.0, 0.0, 1.0) would mean that the observed document would only ever have one topic in case  a three topic probabilistic topic model is build.\n",
    "\n",
    "Usually, one aims to end up with a sample favoring only one topic or a sample that gives a mixture of all topics, or something in between. In case the $\\alpha$-values>1, the samples start to congregate to the center. With greater $\\alpha$ the samples will more likely be uniform or a homogeneous mixture of all the topics.\n",
    "\n",
    "Typically one aims to find more than three topics depending on the number of observed documents. In the ideal case,  the composites to be made up of only a few topics and our parts to belong to only some of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/800/1*_NdnljMqi8L2_lAYwH3JDQ.gif\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cdn-images-1.medium.com/max/800/1*_NdnljMqi8L2_lAYwH3JDQ.gif\", width=700)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA as a  Generative process\n",
    "\n",
    "\n",
    "LDA is being often described as the simplest topic model (...): The intuition behind this model is that documents exhibit multiple topics. Furthermore, one could most easily describe the model by its generative process by which the model assumes the documents in the collection arose (...).\n",
    "\n",
    "Assuming that the word distributions for each topic vary based on a Dirichlet distribution, as do the topic distribution for each document, and the document length is drawn from a Poisson distribution, one can generate the words in a two-stage process for each document in the whole data collection:\n",
    "\n",
    "1. Randomly choose a distribution over topics.\n",
    "2. For each word in the document:\n",
    "   -  A topic is being randomly chosen from the distribution over topics in the first step.\n",
    "      -  Sample parameters for document topic distribution\n",
    "      - $\\theta_{d} \\sim Dirichlet(\\alpha) $\n",
    "   -  A word is being randomly chosen from the corresponding distribution over the vocabulary (For $w=1$ to *W* where *W* is the number of words in document *d*\n",
    "      - Select the topic for word *w* \n",
    "      - $z_{i} \\sim Multinomial(\\theta_{d})$ where $\\theta$ is the topic distribution\n",
    "      - Select word based on topic *z’s* word distribution\n",
    "      - $w_{i} \\sim Multinomial(\\phi^{(z_{i})}) $ where $\\phi$ is the word distributions of each topic\n",
    "\n",
    "The distinctive characteristic of LDA is that all the documents in the collection share the same set of topics and each document exhibits those topics in different proportion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA as a graphical model  \n",
    "LDA can be described more formally with the following notation:\n",
    "\n",
    "<img align=\"middle\" src=\"pic7.png\"> \n",
    "[Source: Blei, 2012.]\n",
    "\n",
    "- The `topics` are $b_{1}:K$, where each $b_{k}$ is a distribution over the vocabulary. \n",
    "\n",
    "- The `topic proportion` for the $d^{th}$ document is anotated by $\\theta_{d}$ where $\\theta_{d,k}$ is the `topic proportion` for `topic` $k$ in `document` $d$.\n",
    "\n",
    "- The `topic assignment` for the $d^{th}$ document is $z_{d}$, where $z_{d,n}$ is the topic assignment for the $n^{th}$  word in document *d*.\n",
    "\n",
    "- The observed `words` for document *d* are $w_{d}$, where $w_{d,n}$ is the $n^{th}$ word in document *d*, which is an element from the fixed vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "Using this notation, the generative process for LDA  is equivalent  to the following joint distribution of the hidden and observed variables (...):\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\beta_{1:K} , \\theta_{1:D} , z_{1:D} , w_{1:D}) =\\displaystyle\\prod_{i=1}^{K}p(\\beta_{i})\\displaystyle\\prod_{d=1}^{D}p(\\theta_{d})\\Bigg( \\displaystyle\\prod_{n=1}^{N}p(z_{d,n}\\mid\\theta_{d}) p(w_{d,n}\\mid\\beta_{1:k}, z_{d,n} )\\Bigg)\n",
    "\\end{equation*}\n",
    "\n",
    "As you can see from this distribution the `topic assignment` $z_{d,n}$\n",
    "depends on the `per-document topic distribution` $\\theta_{d}$, and the word $w_{d,n}$ depends on all of the `topics` $\\beta_{1:K}$ and on the `topic assignment` $z_{d,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Posterior computation for LDA\n",
    "\n",
    "As already mentioned,  \n",
    "> the aim of topic modelling is to automatically discover the topics from a collection of documents, which are observed, while the topic structure is hidden.  \n",
    "\n",
    "This can be thought of as “reversing” the generative process by asking *what is the hidden structure that likely generated the observed collection?* \n",
    "\n",
    "We now turn to the computational problem, computing the conditional distribution of the topic structure given the observed documents (called the *posterior*.) Using our notation, the posterior is \n",
    "\n",
    "$$p(\\theta,\\phi, z \\mid w, \\alpha, \\beta) = \\frac{p(\\theta,\\phi, z, w, \\mid \\alpha, \\beta)}{p(w, \\mid \\alpha, \\beta)}$$\n",
    "\n",
    "The left side of the equation gives us the probability of the document topic distribution, the word distribution of each topic, and the topic labels given all words (in all documents) and the hyperparameters  $\\alpha$ and $\\beta$. In particular, we are interested in estimating the probability of topic (z) for a given word (w) (and our prior assumptions, i.e. hyperparameters) for all words and topics.\n",
    "\n",
    "> ###  A central research goal of modern probabilistic modelling is to develop efficient methods for approximating the posterior inference (Blei, 2012). \n",
    "\n",
    "The general Idea of the Inference process is the following ([Tufts, 2019](https://ldabook.com/index.html)): \n",
    "\n",
    "1. Initialization: Randomly select a topic for each word in each document from a multinomial distribution.\n",
    "2. Gibbs Sampling:\n",
    "   - For *i* iterations\n",
    "   - For document *d* in documents:\n",
    "      - For each word in document *d*:\n",
    "         - assign a topic to the current word based on probability of the topic given the topic of all other words (except the current word).\n",
    "         \n",
    "This procedure is repeated until the samples begin to converge to what would be sampled from the actual distribution. While convergence is theoretically guaranteed with Gibbs Sampling, there is no way of knowing how many iterations are required to reach the stationary distribution ([Darling, 2011](http://u.cs.biu.ac.il/~89-680/darling-lda.pdf)). \n",
    "\n",
    "Whem appling LDA, one is interested in the latent document-topic portions $\\theta_{d}$, the topic-word distributions $\\phi(z)$, and the topic index assignments for each word $z_{i}$  ...... MORE INFORMATION ON COLL. GIBBS SAMPLING FOLLOWS. \n", 
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Similarity Computation\n",
    "### LDA and Jaccard Similarity\n",
    "\n",
    "As already mentioned, we will apply the LDA algorithm to extract the latent topics within\n",
    "Wikipedia's book summaries. This is followed by aggregation of most frequently occurring topics and their utilization in order to calculate the *Jaccard Similarity* between the topics of other book summaries. The combination of latent topics and similarity measurement allows one to uncover both topics and apparent relationships between the summaries of two establishments (Yang and Provinciali, XXXX).\n",
    "\n",
    "The *Jaccard similarity* coefficient measures similarity between two finite sample sets *A* and *B* by counting the number of items they have in common and dividing by the total number of unique items between them:\n",
    "\n",
    "$$ J(A,B)= \\frac{\\mid A \\cap B \\mid}{\\mid A \\cup B\\mid } $$\n",
    "\n",
    "When the *Jaccard similarity* coefficient is above a certain threshold, a high correlation between the latent characteristics of two books is implied.\n",
    "\n",
    ">  However, a content-based recommendation system will not perform on the highest level, if there is no data on user’s preferences, regardless of how detailed our metadata is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "### Building a book recommender in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "** to be updated*\n",
    "\n",
    "Blei, D.M., Ng, A.Y., Jordan, M.I. Latent Dirichlet allocation. *Journal of Machine Learning Research* 3 (2003) 993–1022.\n",
    "\n",
    "Blei, D.M., Lafferty, J.D.: Topic models. Text mining: Classification, clustering, and applications 10 (2009) \n",
    "\n",
    "Griffiths, T.L., Steyvers, M.: Finding scientific topics. Volume 101, Suppl. 1 of *Proceedings of the National Academy of Sciences of the United States of America*. National Academy of Sciences (2004)\n",
    "\n",
    "Mimno, D.M., Hoffman, M.D., Blei, D.M.: Sparse stochastic inference for Latent Dirichlet Allocation. In: *Proceedings of the 29th International Con- ference on Machine Learning (ICML 2012)*. (2012)\n",
    "\n",
    "Chrupala,G.: Efficient induction of probabilistic word classes with LDA. In: *Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP 2011)*. (2011) 363–372\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
